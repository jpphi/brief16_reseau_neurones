{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "floating-review",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import pandas as pds\n",
    "#import string\n",
    "import numpy as np\n",
    "\n",
    "#for machine Learning - classification \n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import confusion_matrix, f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "#\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression, LogisticRegressionCV\n",
    "#\n",
    "# for visuaiization\n",
    "#import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#for NLP\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "entitled-rates",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de message: 40000\n",
      "Les émotions....:\n",
      "      sentiment  compt\n",
      "0        anger    110\n",
      "1      boredom    179\n",
      "2        empty    827\n",
      "3   enthusiasm    759\n",
      "4          fun   1776\n",
      "5    happiness   5209\n",
      "6         hate   1323\n",
      "7         love   3842\n",
      "8      neutral   8638\n",
      "9       relief   1526\n",
      "10     sadness   5165\n",
      "11    surprise   2187\n",
      "12       worry   8459\n",
      "Les auteurs par ordres décroissant du nombre de message postés:\n",
      "                 author  compt\n",
      "7942       MissxMarisa     23\n",
      "2171      ChineseLearn     22\n",
      "7668    MiDesfileNegro     19\n",
      "18330       erkagarcia     19\n",
      "24210         lost_dog     18\n",
      "32187         tsarnick     17\n",
      "6093        KimmiMcfly     15\n",
      "23944      linnetwoods     15\n",
      "3079           Dogbook     14\n",
      "15739      cece_newnew     14\n",
      "26091  mrs_mcsupergirl     14\n",
      "32248       twebbstack     14\n",
      "9312             Quimo     14\n",
      "21495  jesssicababesss     13\n",
      "17396    divxdownloads     13\n",
      "24184     lopezwilfred     12\n",
      "11650     TraceyHewins     12\n",
      "23713        lesley007     12\n",
      "11577           Tittch     11\n",
      "7266      MandyPandy32     11\n",
      "14511      beingnobody     11\n",
      "2393       ComedyQueen     11\n",
      "32704      violetbakes     10\n",
      "2838    DawnofOURnight     10\n",
      "9980           Samm_xo     10\n",
      "3915        Gemmaboyle     10\n",
      "1554    BrandySanDiego     10\n",
      "22742           keza34     10\n",
      "4215    HanaStephenson     10\n",
      "31068     sweetdreamer     10\n"
     ]
    }
   ],
   "source": [
    "# Lecture du fichiern et création de la data frame.\n",
    "#  Nom des colonnes: tweet_id, sentiment, author, content\n",
    "#  Liste des sentiments : 'empty' 'sadness' 'enthusiasm' 'neutral' 'worry' 'surprise' 'love' 'fun'\n",
    "#    'hate' 'happiness' 'boredom' 'relief' 'anger'\n",
    "\n",
    "data_emo2= pds.read_csv('./datas/text_emotion.csv')\n",
    "#data_emo2= pds.read_csv('./datas/text_emotion_02.csv')\n",
    "#, engine= \"python\", encoding='utf-8', error_bad_lines=False) # POUR GOOGLE COLAB SINON ERREUR!\n",
    "\n",
    "print(\"Nombre de message:\",len(data_emo2))\n",
    "\n",
    "df_count_emotion = data_emo2.groupby(['sentiment']).size().reset_index(name='compt')\n",
    "print(\"Les émotions....:\\n\",df_count_emotion)\n",
    "\n",
    "df_count_author = data_emo2.groupby(['author']).size().reset_index(name='compt')\n",
    "#print(df_count_author)\n",
    "print(\"Les auteurs par ordres décroissant du nombre de message postés:\\n\",\n",
    "      df_count_author.sort_values(by= \"compt\", ascending = False).head(30))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "working-blind",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenisation\n",
    "\n",
    "# Création d'une colonne tokenisée\n",
    "corpus=[]\n",
    "for el in data_emo2.content:\n",
    "    token= word_tokenize(el)\n",
    "    corpus.append(token)\n",
    "data_emo2['token_brut'] = pds.Series(corpus)\n",
    "#print(data_emo2.token_brut.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "nonprofit-protein",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'une colonne tokenisée sans stopwords avec ponctuation (important pour les émoticones)\n",
    "stopwords=nltk.corpus.stopwords.words('english')\n",
    "#stopwords.extend(exclude)\n",
    "\n",
    "corpus=[]\n",
    "for el in data_emo2.token_brut:\n",
    "    token=[]\n",
    "    for wd in el:\n",
    "        if wd not in stopwords:\n",
    "            token.append(wd)\n",
    "    corpus.append(token)\n",
    "data_emo2['token_sans_stopwd'] = pds.Series(corpus)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "olive-oklahoma",
   "metadata": {},
   "outputs": [],
   "source": [
    "# À partir de la cellule précédente on ré-écrit les phrase sans stopword (utilisé par la suite \n",
    "#  avec la vectorisation)\n",
    "\n",
    "corpus= data_emo2['token_sans_stopwd']\n",
    "\n",
    "corpus2=[]\n",
    "for el in corpus:\n",
    "    ligne=''\n",
    "    ligne = ' '.join(el) # Magique! \n",
    "    corpus2.append(ligne)\n",
    "\n",
    "data_emo2['phrase_sans_stopwd'] = pds.Series(corpus2)\n",
    "\n",
    "#print(data_emo2.phrase_sans_stopwd.head(2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "damaged-elevation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemantisation et normalisation\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "corpus=[]\n",
    "#print(stopwords)\n",
    "for el in data_emo2.token_sans_stopwd:\n",
    "    token=[]\n",
    "    token=[lemma.lemmatize(word.lower()) for word in el]\n",
    "    corpus.append(token)\n",
    "\n",
    "data_emo2['lemn_normal'] = pds.Series(corpus)\n",
    "\n",
    "#data_emo2.lemn_normal.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "killing-pottery",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "choix= 1\n",
    "\n",
    "if choix== 0:\n",
    "    #vectorisation_cv = CountVectorizer(ngram_range=(1, 2))\n",
    "    vectorisation_cv = CountVectorizer()\n",
    "\n",
    "    cv= vectorisation_cv.fit_transform(data_emo2.phrase_sans_stopwd)\n",
    "    feature_names = vectorisation_cv.get_feature_names() # dictionnaire des mots utilisés\n",
    "    #print(feature_names[:1000])\n",
    "\n",
    "elif choix== 1:\n",
    "    vectorisation_cv= tfidf_vectorizer = TfidfVectorizer()\n",
    "    cv= vectorisation_cv.fit_transform(data_emo2.phrase_sans_stopwd)\n",
    "    feature_names = vectorisation_cv.get_feature_names() # dictionnaire des mots utilisés\n",
    "    #print(feature_names[:1000])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(cv, data_emo2.sentiment, test_size = 0.3,random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "educated-assist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 48117)\t0.33087547086555935\n",
      "  (0, 48118)\t0.33087547086555935\n",
      "  (0, 15660)\t0.31857132044409875\n",
      "  (0, 26330)\t0.2820358400993331\n",
      "  (0, 31585)\t0.26777321863763814\n",
      "  (0, 16792)\t0.2384870969064667\n",
      "  (0, 13691)\t0.2888073027136119\n",
      "  (0, 2364)\t0.3030699241753068\n",
      "  (0, 19080)\t0.20539066264285422\n",
      "  (0, 47572)\t0.16839739703301057\n",
      "  (0, 46878)\t0.18559535337397753\n",
      "  (0, 27770)\t0.24580534492259995\n",
      "  (0, 28013)\t0.1861053783003662\n",
      "  (0, 42966)\t0.13315903581475955\n",
      "  (0, 39353)\t0.1595600446157611\n",
      "  (0, 34973)\t0.148052606916511\n",
      "  (0, 45940)\t0.17611610340707354\n",
      "  (1, 37231)\t0.3225804852145479\n",
      "  (1, 29158)\t0.3580551094809081\n",
      "  (1, 19113)\t0.3823622601643253\n",
      "  (1, 40134)\t0.2952220364648961\n",
      "  (1, 15729)\t0.3117040324366607\n",
      "  (1, 143)\t0.2517103667772666\n",
      "  (1, 12085)\t0.2873968817532436\n",
      "  (1, 40855)\t0.20618579395256692\n",
      "  :\t:\n",
      "  (27997, 39551)\t0.3272866686604793\n",
      "  (27997, 15413)\t0.3888814744962073\n",
      "  (27997, 45857)\t0.2594620429065784\n",
      "  (27997, 18455)\t0.23614169037533383\n",
      "  (27997, 16754)\t0.3024446353432922\n",
      "  (27998, 13489)\t0.3724810497838369\n",
      "  (27998, 20573)\t0.40665481100142264\n",
      "  (27998, 34021)\t0.35993206126889077\n",
      "  (27998, 16682)\t0.25426521020272447\n",
      "  (27998, 33540)\t0.34307493158789604\n",
      "  (27998, 7821)\t0.3656812283625821\n",
      "  (27998, 40548)\t0.28812687117079183\n",
      "  (27998, 42977)\t0.2403103256805843\n",
      "  (27998, 26587)\t0.19216747663519929\n",
      "  (27998, 45116)\t0.19054314318970608\n",
      "  (27998, 4918)\t0.19042978212318062\n",
      "  (27999, 20604)\t0.41963850588748225\n",
      "  (27999, 5951)\t0.41963850588748225\n",
      "  (27999, 25234)\t0.3773567166788943\n",
      "  (27999, 33256)\t0.31807031220247517\n",
      "  (27999, 39864)\t0.27488291549091887\n",
      "  (27999, 5942)\t0.3224319415351002\n",
      "  (27999, 42966)\t0.337762474132922\n",
      "  (27999, 41635)\t0.2604902399359417\n",
      "  (27999, 30248)\t0.20682989493936446\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "stone-theology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 62, 16, 32, 32]\n",
      "[3008, 436, 400, 3052, 258, 3052, 2927, 337, 53, 1668, 6, 34, 20, 1]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "import string\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_emo2.phrase_sans_stopwd, data_emo2.sentiment, test_size = 0.3,random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(num_words=5000, lower= False, filters= '')\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_we = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_we = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
    "\n",
    "print(X_train_we[2])\n",
    "print(X_test_we[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "sharp-chile",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45854"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ultimate-client",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: \"ehe 's option ( advertising ) world n't mess ! So I might future ! We 're time yes . Lï¿½get dï¿½g ?\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-7de60b4cedb2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmaxlen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/preprocessing/sequence.py\u001b[0m in \u001b[0;36mpad_sequences\u001b[0;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[1;32m    154\u001b[0m           \u001b[0;32mor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcase\u001b[0m \u001b[0mof\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m   \"\"\"\n\u001b[0;32m--> 156\u001b[0;31m   return sequence.pad_sequences(\n\u001b[0m\u001b[1;32m    157\u001b[0m       \u001b[0msequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m       padding=padding, truncating=truncating, value=value)\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/keras_preprocessing/sequence.py\u001b[0m in \u001b[0;36mpad_sequences\u001b[0;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;31m# check `trunc` has expected shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mtrunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0msample_shape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             raise ValueError('Shape of sample %s of sequence at position %s '\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: \"ehe 's option ( advertising ) world n't mess ! So I might future ! We 're time yes . Lï¿½get dï¿½g ?\""
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "maxlen = 100\n",
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "applicable-strike",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "velvet-battle",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'zeros' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-a4cdf9bca582>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0membedding_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0membedding_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membeddings_dictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0membedding_vector\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0membedding_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'zeros' is not defined"
     ]
    }
   ],
   "source": [
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "exterior-auditor",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Embedding' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-ffb0f48b1b7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdeep_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0membedding_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0membedding_matrix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeep_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mLSTM_Layer_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdense_layer_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM_Layer_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Embedding' is not defined"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input\n",
    "\n",
    "deep_inputs = Input(shape=(maxlen,))\n",
    "embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], trainable=False)(deep_inputs)\n",
    "LSTM_Layer_1 = LSTM(128)(embedding_layer)\n",
    "dense_layer_1 = Dense(6, activation='sigmoid')(LSTM_Layer_1)\n",
    "model = Model(inputs=deep_inputs, outputs=dense_layer_1)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endangered-cooperative",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
